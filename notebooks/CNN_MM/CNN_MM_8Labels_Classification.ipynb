{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Multiclass classification : Anorexia related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = {\n",
    "    \"test\": \"./path/to/test.csv\",\n",
    "    \"training\": \"./path/to/train.csv\",\n",
    "    \"dev\": \"./path/to/dev.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path, ty=\"main_label\"):\n",
    "    data = open(path, \"rb\").read().decode(\"utf-8\").strip()\n",
    "    labels = []\n",
    "    entities = []\n",
    "    texts = []\n",
    "    for instance in data.split(\"####END-POST####\"):\n",
    "        if not instance==\"\":\n",
    "            instance = instance.replace(\"####POST####\\n\", \"\")\n",
    "            label = int(float(instance.split(\"####TITLE####\")[0].strip().split(\"\\n\")[0]))\n",
    "            if ty == \"binary\":\n",
    "                label = int(label < 7)\n",
    "            t = instance.split(\"####TEXT####\")[1].strip()\n",
    "            text = []\n",
    "            ent = []\n",
    "            for x in t.split(\"\\n\"):\n",
    "                if not x==\"\":\n",
    "                    text.append(x.split(\"\\t\")[0])\n",
    "                    ent.append(x.split(\"\\t\")[1])\n",
    "            entities.append(ent)\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "    return {\"entities\": entities, \"texts\": texts, \"labels\": labels}\n",
    "training = read_corpus(path_corpus[\"training\"], \"main_label\")\n",
    "dev = read_corpus(path_corpus[\"dev\"], \"main_label\")\n",
    "test = read_corpus(path_corpus[\"test\"], \"main_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_data(entities, text, dict_entities=None, word2index=None, modify=True):\n",
    "    dict_entities = {} if dict_entities is None else dict_entities\n",
    "    entities_c = []\n",
    "    for x in entities:\n",
    "        entities_c_l = []\n",
    "        for y in x:\n",
    "            if not y in dict_entities and modify:\n",
    "                dict_entities[y] = len(dict_entities)\n",
    "            if not y in dict_entities:\n",
    "                dict_entities[y] = dict_entities[\"__UNK__\"]\n",
    "            entities_c_l.append(dict_entities[y])\n",
    "        entities_c.append(entities_c_l)\n",
    "    \n",
    "    text_c = []\n",
    "    for x in text:\n",
    "        text_c.append([0 if not y in word2index else word2index[y] for y in x])\n",
    "    return entities_c, text_c, dict_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "max_seq_len = 500\n",
    "\n",
    "e, full_vocab = pickle.load(open(\"resources/embedding.pkl\",\"rb\"))\n",
    "e = np.vstack((np.zeros((1, e.shape[1])),\n",
    "                            np.random.uniform(-0.25, 0.25, e.shape[1]), e))\n",
    "full_vocab[\"__UNK__\"] = 0\n",
    "full_vocab[\"__PAD__\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities, train_p, d_ent = cast_data(training[\"entities\"], training[\"texts\"], dict_entities={\"__UNK__\":0}, word2index=full_vocab)\n",
    "dev_entities, dev_p, d_ent = cast_data(dev[\"entities\"], dev[\"texts\"], dict_entities=d_ent, word2index=full_vocab, modify=False)\n",
    "test_entities, test_p, d_ent = cast_data(test[\"entities\"], test[\"texts\"], dict_entities=d_ent, word2index=full_vocab, modify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "d_ent[\"__PAD__\"] = len(d_ent)\n",
    "\n",
    "# pad sequence\n",
    "train_entities = pad_sequences(train_entities, maxlen=max_seq_len, padding=\"post\", value=d_ent[\"__PAD__\"])\n",
    "dev_entities = pad_sequences(dev_entities, maxlen=max_seq_len, padding=\"post\", value=d_ent[\"__PAD__\"])\n",
    "test_entities = pad_sequences(test_entities, maxlen=max_seq_len, padding=\"post\", value=d_ent[\"__PAD__\"])\n",
    "\n",
    "train_p = pad_sequences(train_p, maxlen=max_seq_len, padding=\"post\", value=full_vocab[\"__PAD__\"])\n",
    "dev_p = pad_sequences(dev_p, maxlen=max_seq_len, padding=\"post\", value=full_vocab[\"__PAD__\"])\n",
    "test_p = pad_sequences(test_p, maxlen=max_seq_len, padding=\"post\", value=full_vocab[\"__PAD__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "y_train = to_categorical(training[\"labels\"], 8)\n",
    "y_dev = to_categorical(dev[\"labels\"], 8)\n",
    "y_test = to_categorical(test[\"labels\"], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pattern_input (InputLayer)      (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 500, 200)     3455600     pattern_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 500, 50)      5000        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 500, 250)     0           embedding_11[0][0]               \n",
      "                                                                 embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 496, 128)     160128      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            1032        global_max_pooling1d_6[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,621,760\n",
      "Trainable params: 166,160\n",
      "Non-trainable params: 3,455,600\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3289 samples, validate on 985 samples\n",
      "Epoch 1/20\n",
      "3289/3289 [==============================] - 61s 18ms/step - loss: 0.6927 - acc: 0.8161 - val_loss: 0.6492 - val_acc: 0.8061\n",
      "Epoch 2/20\n",
      "3289/3289 [==============================] - 42s 13ms/step - loss: 0.4935 - acc: 0.8565 - val_loss: 0.6004 - val_acc: 0.8203\n",
      "Epoch 3/20\n",
      "3289/3289 [==============================] - 51s 16ms/step - loss: 0.3661 - acc: 0.8924 - val_loss: 0.5811 - val_acc: 0.8193\n",
      "Epoch 4/20\n",
      "3289/3289 [==============================] - 47s 14ms/step - loss: 0.2635 - acc: 0.9319 - val_loss: 0.5502 - val_acc: 0.8274\n",
      "Epoch 5/20\n",
      "3289/3289 [==============================] - 39s 12ms/step - loss: 0.1860 - acc: 0.9629 - val_loss: 0.5610 - val_acc: 0.8335\n",
      "Epoch 6/20\n",
      "3289/3289 [==============================] - 40s 12ms/step - loss: 0.1384 - acc: 0.9763 - val_loss: 0.5687 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.1.\n",
      "Epoch 7/20\n",
      "3289/3289 [==============================] - 38s 11ms/step - loss: 0.0977 - acc: 0.9842 - val_loss: 0.6118 - val_acc: 0.8254\n",
      "Epoch 8/20\n",
      "3289/3289 [==============================] - 40s 12ms/step - loss: 0.0939 - acc: 0.9845 - val_loss: 0.6202 - val_acc: 0.8294\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(500)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, Bidirectional, Input\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.initializers import glorot_normal\n",
    "\n",
    "X_train = [train_entities, train_p]\n",
    "X_dev = [dev_entities, dev_p]\n",
    "X_test = [test_entities, test_p]\n",
    "\n",
    "x_input = Input(shape=(max_seq_len,), dtype='float32', name='pattern_input')\n",
    "embedding_layer = Embedding(input_dim=e.shape[0],\n",
    "                            output_dim=e.shape[1],\n",
    "                            weights=[e], trainable=False) \n",
    "word = embedding_layer(x_input)\n",
    "ent_input = Input(shape=(max_seq_len,))\n",
    "ent = Embedding(input_dim=len(d_ent), output_dim=50, trainable=True)(ent_input)\n",
    "\n",
    "x = concatenate([word, ent])\n",
    "x = Convolution1D(128, 5, padding=\"valid\", strides=1, activation='tanh')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = Dense(8, activation='softmax')(x) \n",
    "model = Model(inputs=[ent_input, x_input], outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adadelta\", metrics=['acc'])\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(patience=2, min_lr=0.0001, verbose=1, factor=0.1),\n",
    "    EarlyStopping(patience=4, verbose=1)]\n",
    "history = model.fit(X_train, y_train, epochs=20, \n",
    "                    batch_size=2,\n",
    "                    shuffle=True, verbose=1, callbacks=callbacks, validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.59      0.61        17\n",
      "           1       0.50      0.07      0.12        56\n",
      "           2       0.50      0.20      0.29         5\n",
      "           3       0.17      0.04      0.06        26\n",
      "           4       0.59      0.54      0.57        24\n",
      "           5       0.65      0.43      0.51        47\n",
      "           6       0.22      0.14      0.17        14\n",
      "           7       0.86      0.98      0.92       645\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       834\n",
      "   macro avg       0.51      0.37      0.41       834\n",
      "weighted avg       0.77      0.82      0.78       834\n",
      "\n",
      "[[ 10   1   1   0   0   4   1   0]\n",
      " [  2   4   0   3   2   2   3  40]\n",
      " [  1   0   1   0   1   1   0   1]\n",
      " [  0   1   0   1   0   0   0  24]\n",
      " [  2   0   0   0  13   1   0   8]\n",
      " [  1   0   0   0   2  20   0  24]\n",
      " [  0   2   0   0   1   0   2   9]\n",
      " [  0   0   0   2   3   3   3 634]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.46      0.51        24\n",
      "           1       0.78      0.16      0.27        43\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.50      0.17      0.25        36\n",
      "           4       0.49      0.46      0.47        39\n",
      "           5       0.68      0.52      0.59        48\n",
      "           6       0.25      0.25      0.25         8\n",
      "           7       0.87      0.98      0.92       765\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       985\n",
      "   macro avg       0.52      0.37      0.41       985\n",
      "weighted avg       0.79      0.83      0.80       985\n",
      "\n",
      "[[ 11   0   0   0   2   3   1   7]\n",
      " [  6   7   0   5   2   0   2  21]\n",
      " [  1   1   0   0   2   2   3  13]\n",
      " [  0   0   0   6   0   0   0  30]\n",
      " [  1   0   0   0  18   3   0  17]\n",
      " [  0   1   0   0   2  25   0  20]\n",
      " [  0   0   0   0   0   0   2   6]\n",
      " [  0   0   1   1  11   4   0 748]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_test_classes_pred = y_pred_test.argmax(axis=-1)\n",
    "\n",
    "#y_pred_train = model.predict(X_train)\n",
    "#y_train_classes_pred = y_pred_train.argmax(axis=-1)\n",
    "\n",
    "y_pred_dev = model.predict(X_dev)\n",
    "y_dev_classes_pred = y_pred_dev.argmax(axis=-1)\n",
    "\n",
    "print(classification_report(test[\"labels\"], y_test_classes_pred))\n",
    "print(confusion_matrix(test[\"labels\"], y_test_classes_pred))\n",
    "\n",
    "print(classification_report(dev[\"labels\"], y_dev_classes_pred))\n",
    "print(confusion_matrix(dev[\"labels\"], y_dev_classes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final-models/mc-cnn-keys+embeddings.pkl')\n",
    "pickle.dump(d_ent, open(\"final-models/dict_mc.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
