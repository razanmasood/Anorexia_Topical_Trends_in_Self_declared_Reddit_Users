{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import activations\n",
    "from keras.layers import Dense, Embedding, Softmax, Dropout, Input, Concatenate\n",
    "from keras.layers import LSTM, GlobalAveragePooling1D\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "import keras_metrics as km\n",
    "\n",
    "#from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590\n"
     ]
    }
   ],
   "source": [
    "# Terms\n",
    "\n",
    "set_0 = {'rate', 'semester', '15th', 'clarity', 'noooope', 'cardiac', 'deadline', 'isolated', 'thrive', 'realize', 'meal', 'asian', 'relapse', 'refresh', 'diagnose', 'frequency', 'maintain', '8ounces', 'psychologist', 'caregiver', 'caution', '3however', 'heart', 'alienation', 'km', 'woe', 'imo', 'mellow', 'cigarette', 'desperately', 'discernable', 'hike', 'passion', 'perseverance', 'ptsd', 'scrapbooke', 'nmother', 'elevate', 'lightheaded', 'seek', 'ignore', 'residential', 'body', 'triggering', 'frustrate', 'south', 'disorders', 'symptom', 'restricting', 'saddle', 'october', 'afloat', 'isolate', 'fathom', 'continuous', 'severe', 'ride', 'buddy', 'ednos', 'hierarchy', 'inpatient', '144', 'passionate', 'unbearably', 'electrolyte', 'accountable', 'fridays', 'misunderstood', 'fast', 'batshit', 'proed', 'pound', 'unstable', 'mechanism', 'healthy', 'weigh', 'lamictal', 'struggle', '93', 'garment', 'vyvanse', 'dietician', 'unattractive', 'hyperthyroidism', 'leotard', 'dysmorphia', 'weight', 'apostrophe', 'clarita', 'recovery', 'anorexic', 'bulimia', 'specialize', 'mpa', 'therapist', 'recover', 'eat', 'ed', 'anorexia', 'disorder'}\n",
    "set_1 = {'adios', 'brendajames', 'darkslay', 'gt', 'halfbro', 'inactive', 'pikaokulucky7', 'shoutouts', 'soccrmom', 'stabbd', 'stale', 'virgoth', 'upwards', 'strategy', 'economic', 'print', 'negative', 'cheesecake', 'furyball', 'noooo', 'whisky', 'hetero', 'pathologist', 'malnourishment', 'february', 'delicate', 'aldc', 'anx', 'authority', 'dissociate', 'peer', 'seeing', 'pharmacology', 'successful', 'ad', 'memorize', 'commit', 'shrink', 'identity', 'booze', 'lightweight', 'liquor', 'poisoning', 'swig', 'shelter', 'flashback', 'linger', 'uncommon', 'behavioral', 'dismiss', 'sheer', 'suggestive', 'drug', 'kite', 'tamaz', 'valium', 'apache', 'helicopter', 'partizan', 'shitshow', 'unstoppable', 'unable', 'massage', 'alcoholic', 'takin', 'beg', 'crying', 'sarcastic', 'uptight', 'pms', 'wheelchair', 'stress', 'patien', 'insomnia', 'noticeably', 'pantie', 'drinking', 'attachment', 'smoking', 'diary', 'economics', 'bum', 'oman', 'harm', 'paige', 'charliehorse', 'suicidal', 'calmly', 'sleep', 'cupple', 'asleep', 'traumatic', 'dissociative', 'daydream', 'emo', 'anxiety', 'depressed', 'scar', 'anxious', 'depression'}\n",
    "set_2 = {'urine', 'stone', 'climb', 'truely', 'fearbola', 'insertion', 'headache', 'syndrome', 'clinic', 'diagnose', 'cancer', 'germ', 'drained', 'headed', 'likewise', 'seene', 'eveybody', 'revise', 'awful', 'airborne', 'droplet', 'particle', 'immune', 'senstive', 'appointment', 'blind', 'furbabie', 'slobbery', 'volunteer', 'fatigue', 'fibromyalgia', 'autistic', 'mute', 'watte', 'standpoint', 'encouragement', 'deal', 'crohn', 'informed', 'suffer', 'aspiration', 'inflammation', 'tarde', 'pig', 'remark', 'aids', 'nausea', 'doctor', 'hospitalize', 'autism', 'xenophobia', 'understate', 'contract', 'relative', 'runny', 'foodborne', 'salmonella', 'viral', 'africa', 'discharge', 'infectious', 'painful', 'est', 'pep', 'facility', 'surgery', 'unerected', 'kidney', 'monger', 'healing', 'pity', 'cure', 'cramp', 'outbreak', 'asthma', 'sinus', 'sickness', 'transfusion', 'fever', 'thyroid', 'infection', 'pneumonia', 'blindness', 'staff', 'spread', 'neb', 'pain', 'uncurable', 'therapist', 'virus', 'pcp', 'ibs', 'hiv', 'endo', 'appt', 'therpist', 'hrt', 'flu', 'disease', 'ebola'}\n",
    "set_3 = {'demeaning', 'regrow', 'snag', 'spool', 'entp', 'abuse', 'familiar', 'text', 'married', 'aide', 'od', 'oxygen', 'sarcoidosis', 'flaky', 'unreliable', 'grieve', 'partner', 'behaviour', 'genuinely', 'trend', 'girlfriend', 'sexually', 'mother', 'break', 'hopeless', 'sponge', 'torment', 'asd', 'authentic', 'cosplayer', 'fulfil', 'maximus', 'chair', 'kid', 'curriculum', 'anger', 'old', 'friend', 'assumption', 'communicate', 'atm', 'dilemma', 'express', 'isle', 'gentleman', 'shy', 'marry', 'notch', 'flow', 'impulsive', 'colourblind', 'courageous', 'shaven', 'typo', 'electric', 'divorce', 'sexual', 'snoop', 'mom', 'insensitive', 'son', 'grin', 'husband', 'client', 'incredibly', 'dad', 'agao', 'newly', 'asda', 'beverly', 'hills', 'housewives', 'thanksgiving', 'license', 'impression', 'google', 'humiliation', 'ex', 'infj', 'abusive', 'favour', 'unfulfilling', 'enraging', 'snap', 'tell', 'toothbrush', 'denial', 'father', 'molester', 'boyfriend', 'tickle', 'upfront', 'dfw', 'daycare', 'immature', 'sister', 'child', 'daughter', 'relationship', 'parent'}\n",
    "set_4 = {'serve', '209', 'omelet', 'hummus', '32', 'portion', 'plan', 'bran', 'blend', 'sauce', 'fast', 'jalfrezi', 'spelt', 'pretzel', 'onion', 'pesto', 'salt', 'nothinglunch', 'lemon', 'cookies', 'cracker', 'fiberone', '140', '150', '28', 'banana', 'cherry', 'patty', 'omega', 'stevia', 'caffeine', 'mini', 'bar', 'horseback', 'trek', 'veggie', 'garlic', 'multi', 'unsweetened', 'vegetarian', 'vegan', 'mix', 'white', 'frozen', 'rice', 'carrot', 'sausage', 'fiber', '34', 'sandwich', 'zucchini', 'oz', 'pasta', 'vanilla', 'spinach', 'food', 'hungry', 'cinnamon', 'bean', 'joe', 'trader', 'potato', 'fridge', '100', 'autumn', 'yogurt', 'fruit', 'cheese', 'bread', 'bagel', 'truvia', 'meal', 'almond', 'mg', 'greek', 'tablespoon', 'snack', 'cream', 'coffee', 'milk', 'eat', 'peanut', 'calorie', 'curry', 'quest', 'cereal', 'butter', 'stack', 'lunch', '200', 'protein', 'dinner', 'cal', 'powder', 'egg', 'cup', 'fage', 'total', 'chicken', 'breakfast'}\n",
    "set_5 = {'compulsively', 'dishonest', 'restrict', 'mentality', 'underweight', 'wight', 'happiness', 'whoops', 'borderline', 'weighing', '2200', 'fist', 'welp', 'gratifying', 'eveeeer', '40lbs', 'dainty', 'cardio', 'eatingdisorder', '5k', 'f22', '181', 'unnatractive', 'morbidly', 'biking', 'bulky', '188', 'cw', 'plateaus', '256', 'look', '22', '122', 'promote', '158', 'wl', 'expression', 'classify', 'eighteen', '148', 'liar', '65', 'strenuous', 'yeeeah', 'starting', 'sizes', '270', '96', 'scale', 'lw', 'flattering', '110', 'mirror', 'exercise', 'waist', 'thigh', 'big', 'feel', '96lbs', 'yo', '107', 'loss', 'healthy', '30e', 'excess', 'need', 'scare', 'corset', 'tall', 'hw', 'sexy', '92', '132', '170', 'skinny', '123', 'ft', 'ugw', 'cm', 'body', 'ss', '22f', '10lbs', 'weigh', 'unhealthy', '120', 'lbs', 'muscle', 'obese', 'overweight', 'size', 'cus', 'height', 'lb', 'bmi', 'gain', 'pound', 'fat', 'lose', 'weight'}\n",
    "set_6 = {'urine', 'stone', 'climb', 'truely', 'fearbola', 'insertion', 'headache', 'syndrome', 'clinic', 'diagnose', 'cancer', 'germ', 'drained', 'headed', 'likewise', 'seene', 'eveybody', 'revise', 'awful', 'airborne', 'droplet', 'particle', 'immune', 'senstive', 'appointment', 'blind', 'furbabie', 'slobbery', 'volunteer', 'fatigue', 'fibromyalgia', 'autistic', 'mute', 'watte', 'standpoint', 'encouragement', 'deal', 'crohn', 'informed', 'suffer', 'aspiration', 'inflammation', 'tarde', 'pig', 'remark', 'aids', 'nausea', 'doctor', 'hospitalize', 'autism', 'xenophobia', 'understate', 'contract', 'relative', 'runny', 'foodborne', 'salmonella', 'viral', 'africa', 'discharge', 'infectious', 'painful', 'est', 'pep', 'facility', 'surgery', 'unerected', 'kidney', 'monger', 'healing', 'pity', 'cure', 'cramp', 'outbreak', 'asthma', 'sinus', 'sickness', 'transfusion', 'fever', 'thyroid', 'infection', 'pneumonia', 'blindness', 'staff', 'spread', 'neb', 'pain', 'uncurable', 'therapist', 'virus', 'pcp', 'ibs', 'hiv', 'endo', 'appt', 'therpist', 'hrt', 'flu', 'disease', 'ebola'}\n",
    "pattern_set= set_0.union(set_1, set_2, set_3, set_4, set_5, set_6)\n",
    "\n",
    "\n",
    "print(len(pattern_set))\n",
    "patterns = list(pattern_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data contains posts that are only 70 tokens length. If the post is longet then only the head and the tail \n",
    "#(35 tokens length each) are concidered\n",
    "train_df = pd.read_csv(directory+'train.csv')\n",
    "dev_df = pd.read_csv(directory+'dev.csv')\n",
    "test_df = pd.read_csv(directory+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df.short_posts\n",
    "train_words = [ nltk.word_tokenize( str(post) ) for post in train_text ]\n",
    "X_content_train = train_words\n",
    "print(X_content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text = train_df.short_posts\n",
    "train_words = [ nltk.word_tokenize( str(post) ) for post in train_text ]\n",
    "X_content_train = train_words\n",
    "print(X_content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_df.short_posts\n",
    "test_words = [ nltk.word_tokenize( str(post) ) for post in test_text ]\n",
    "X_content_test = test_words\n",
    "print(X_content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10475\n",
      "Most frequent tokens\n",
      "\tnot: 1187\n",
      "\tget: 731\n",
      "\tlike: 676\n",
      "\twould: 584\n",
      "\tgo: 572\n",
      "\tthink: 479\n",
      "\tknow: 477\n",
      "\tmake: 423\n",
      "\treally: 405\n",
      "\tone: 402\n",
      "Least frequent tokens\n",
      "\tcharliehorse: 1\n",
      "\t123: 1\n",
      "\tcaregiver: 1\n",
      "\tcrohn: 1\n",
      "\tproed: 1\n",
      "\tshaven: 1\n",
      "\tjoe: 1\n",
      "\tflow: 1\n",
      "\ttarde: 1\n",
      "\tuptight: 1\n",
      "Max. Sequence Length: 70\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "max_seq_len = 0\n",
    "EMBEDDING_DIM = 300\n",
    "UNK_TOKEN = '__unk__'\n",
    "#max_seq = None\n",
    "\n",
    "#Here the code is dealing with tweets and takes the maximum length of a tweet as input\n",
    "#This should be changed to work with reddit posts which are longer.\n",
    "#A suggestion is to consider the avg length and take the beginning and the tail of a lengthy post\n",
    "\n",
    "full_vocab = dict()\n",
    "for instance in X_content_train:\n",
    "    max_seq_len = max(max_seq_len, len(instance))\n",
    "    for token in instance:\n",
    "        full_vocab[token] = 1 + full_vocab.get(token, 0)\n",
    "        \n",
    "for instance in X_content_dev:\n",
    "    max_seq_len = max(max_seq_len, len(instance))   \n",
    "\n",
    "for instance in X_content_test:\n",
    "    max_seq_len = max(max_seq_len, len(instance))\n",
    "    \n",
    "for voc in patterns:\n",
    "    full_vocab[voc] = 1 + full_vocab.get(voc, 0)  \n",
    "# Sort vocabulary by occurrence\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\n",
    "\n",
    "# Print some samples\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\n",
    "print(\"Most frequent tokens\")\n",
    "for i in range(10):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\n",
    "print(\"Least frequent tokens\")\n",
    "for i in range(1,11):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\n",
    "\n",
    "if len(sorted_vocab) >= 10000:\n",
    "    vocab_size = 10000\n",
    "else:\n",
    "    vocab_size = len(sorted_vocab)\n",
    "print(\"Max. Sequence Length: %d\"%(max_seq_len))\n",
    "#print max_seq\n",
    "# Create final vocab\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:vocab_size])}\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:vocab_size])}\n",
    "\n",
    "word2idx[UNK_TOKEN] = vocab_size\n",
    "idx2word[vocab_size] = UNK_TOKEN\n",
    "vocab_size = vocab_size + 1\n",
    "\n",
    "X_content_train = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in X_content_train]\n",
    "X_content_dev = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in X_content_dev]\n",
    "X_content_test = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in X_content_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 459/65473\n",
      "Dev: 2831/23066\n",
      "Test: 2515/20651\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens filtered out as unknown:\")\n",
    "print(\"Train: %d/%d\"%(len([1 for instance in X_content_train for t in instance if t == UNK_TOKEN]), sum([len(i) for i in X_content_train])))\n",
    "print(\"Dev: %d/%d\"%(len([1 for instance in X_content_dev for t in instance if t == UNK_TOKEN]), sum([len(i) for i in X_content_dev])))\n",
    "print(\"Test: %d/%d\"%(len([1 for instance in X_content_test for t in instance if t == UNK_TOKEN]), sum([len(i) for i in X_content_test])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_to_tensor(data, pad_value=vocab_size):\n",
    "    # First convert from words to indices\n",
    "    idx_data = [[word2idx[t] for t in instance] for instance in data]\n",
    "    \n",
    "    # Create numpy representation\n",
    "    return pad_sequences([np.array(d) for d in idx_data], maxlen=max_seq_len, value=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_content_train = data_to_tensor(X_content_train)\n",
    "X_content_dev = data_to_tensor(X_content_dev)\n",
    "X_content_test = data_to_tensor(X_content_test)\n",
    "vocab_size = vocab_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8695/10001 pre-trained vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove/', 'glove.42B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "#print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "found = 0\n",
    "embedding_matrix = np.zeros((len(word2idx) + 1, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found += 1        \n",
    "        \n",
    "print(\"Loaded %d/%d pre-trained vectors\"%(found, len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.main_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0    2549\n",
      "5.0     188\n",
      "4.0     171\n",
      "6.0     108\n",
      "0.0      85\n",
      "3.0      84\n",
      "1.0      71\n",
      "2.0      33\n",
      "Name: main_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.main_label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0    765\n",
      "5.0     48\n",
      "1.0     43\n",
      "4.0     39\n",
      "3.0     36\n",
      "0.0     24\n",
      "2.0     22\n",
      "6.0      8\n",
      "Name: main_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_dev = dev_df.main_label.values\n",
    "print(dev_df.main_label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0    645\n",
      "1.0     56\n",
      "5.0     47\n",
      "3.0     26\n",
      "4.0     24\n",
      "0.0     17\n",
      "6.0     14\n",
      "2.0      5\n",
      "Name: main_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_test = test_df.main_label.values\n",
    "print(test_df.main_label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train labels:  3289\n",
      "Number of dev labels:  985\n",
      "Number of test labels:  834\n",
      "(3289, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train labels: \", len(y_train))\n",
    "print(\"Number of dev labels: \", len(y_dev))\n",
    "print(\"Number of test labels: \", len(y_test))\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = to_categorical(encoded_y_train)\n",
    "\n",
    "encoder.fit(y_dev)\n",
    "encoded_y_dev = encoder.transform(y_dev)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_dev = to_categorical(encoded_y_dev)\n",
    "\n",
    "encoder.fit(y_test)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = to_categorical(encoded_y_test)\n",
    "\n",
    "print(dummy_y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention layer class\n",
    "class InnerAttention(Layer):\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        \"\"\"\n",
    "        # Input shapes\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        \"\"\"\n",
    "        self.return_attention = return_attention\n",
    "        super(InnerAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shapes):\n",
    "        X, u = input_shapes\n",
    "        print(\"input shapes\", input_shapes)\n",
    "        # Save some parameters for future convinience\n",
    "        self.seq_length = X[1]\n",
    "        self.embed_size = X[2]\n",
    "\n",
    "        # Assumes embedding size of X and u is the same\n",
    "        self.W = self.add_weight(name='kernel',\n",
    "                                      shape=(self.embed_size, self.embed_size),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(InnerAttention, self).build(input_shapes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        X, u = inputs\n",
    "\n",
    "        # Compute u' . W_s\n",
    "        uW = K.dot(u, self.W)\n",
    "\n",
    "        # Compute u' . W_s . X\n",
    "        uWX = K.batch_dot(uW, X, axes=[1,2])\n",
    "        \n",
    "        # Apply Sigmoid\n",
    "        alpha = K.sigmoid(uWX)\n",
    "        \n",
    "        # Weigh each element of matrix X using alpha\n",
    "        result = inputs[0] * K.reshape(alpha, (-1, self.seq_length, 1))\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, alpha]\n",
    "        return result\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        X_shape, _ = input_shapes\n",
    "        if self.return_attention:\n",
    "            return [X_shape, X_shape[1]]\n",
    "        else:\n",
    "            return X_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size : 10002\n",
      "length word2idx : 10001\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab_size :\", vocab_size)\n",
    "print(\"length word2idx :\", len(word2idx))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)  \n",
    "\n",
    "pattern_data = np.array([word2idx[t.lower()] for t in patterns if t.lower() in word2idx])\n",
    "pattern_size = len(pattern_data)\n",
    "        \n",
    "train_size = len(y_train)\n",
    "dev_size = len(y_dev)\n",
    "test_size = len(y_test)\n",
    "\n",
    "X_pattern_train = np.zeros((train_size,pattern_size))\n",
    "for i in range(train_size):\n",
    "    X_pattern_train[i] = pattern_data\n",
    "    \n",
    "X_pattern_dev = np.zeros((dev_size,pattern_size))\n",
    "for i in range(dev_size):\n",
    "    X_pattern_dev[i] = pattern_data  \n",
    "    \n",
    "X_pattern_test = np.zeros((test_size,pattern_size))\n",
    "for i in range(test_size):\n",
    "    X_pattern_test[i] = pattern_data      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [X_content_train, X_pattern_train]\n",
    "X_dev = [X_content_dev, X_pattern_dev] \n",
    "X_test = [X_content_test, X_pattern_test]\n",
    "\n",
    "\n",
    "pattern_input = Input(shape=(pattern_size,), dtype='float32', name='pattern_input')\n",
    "pattern_embed = embedding_layer(pattern_input)\n",
    "pattern_embed = GlobalAveragePooling1D()(pattern_embed) \n",
    "\n",
    "tweet_sentence_input = Input(shape=(max_seq_len,), dtype='int32', name='tweet_sentence_input')\n",
    "tweet_embed = embedding_layer(tweet_sentence_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shapes [(None, 70, 300), (None, 300)]\n",
      "input shapes [(None, 70, 300), (None, 300)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "\n",
    "x, attn = InnerAttention(return_attention=True)([tweet_embed, pattern_embed])\n",
    "x = InnerAttention()([tweet_embed, pattern_embed])  \n",
    "x = LSTM(36, \n",
    "         #recurrent_dropout=0.01\n",
    "        )(x)\n",
    "x = Dropout(0.01)(x)\n",
    "x = Dense(8, activation='softmax')(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "tweet_sentence_input (InputLaye (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pattern_input (InputLayer)      (None, 574)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         multiple             3000600     pattern_input[0][0]              \n",
      "                                                                 tweet_sentence_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 300)          0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "inner_attention_18 (InnerAttent (None, 70, 300)      90000       embedding_5[1][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 36)           48528       inner_attention_18[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 36)           0           lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 8)            296         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,139,424\n",
      "Trainable params: 138,824\n",
      "Non-trainable params: 3,000,600\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3289 samples, validate on 985 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 1.6846 - acc: 0.6047 - val_loss: 1.3598 - val_acc: 0.7675\n",
      "Epoch 2/50\n",
      " - 5s - loss: 1.2613 - acc: 0.7692 - val_loss: 1.1509 - val_acc: 0.7766\n",
      "Epoch 3/50\n",
      " - 5s - loss: 1.1060 - acc: 0.7750 - val_loss: 1.0563 - val_acc: 0.7766\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.9348 - acc: 0.7756 - val_loss: 0.9307 - val_acc: 0.7766\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.8282 - acc: 0.7829 - val_loss: 0.8461 - val_acc: 0.7777\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.7477 - acc: 0.7942 - val_loss: 0.7994 - val_acc: 0.7766\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.7083 - acc: 0.8015 - val_loss: 0.7747 - val_acc: 0.7746\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.6774 - acc: 0.8039 - val_loss: 0.7588 - val_acc: 0.7787\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.6564 - acc: 0.8075 - val_loss: 0.7463 - val_acc: 0.7817\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.6372 - acc: 0.8121 - val_loss: 0.7381 - val_acc: 0.7838\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.6237 - acc: 0.8148 - val_loss: 0.7335 - val_acc: 0.7858\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.6075 - acc: 0.8206 - val_loss: 0.7267 - val_acc: 0.7939\n",
      "Epoch 13/50\n",
      " - 5s - loss: 0.5974 - acc: 0.8224 - val_loss: 0.7281 - val_acc: 0.7980\n",
      "Epoch 14/50\n",
      " - 5s - loss: 0.5850 - acc: 0.8264 - val_loss: 0.7204 - val_acc: 0.7959\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.5708 - acc: 0.8316 - val_loss: 0.7139 - val_acc: 0.7980\n",
      "Epoch 16/50\n",
      " - 5s - loss: 0.5596 - acc: 0.8349 - val_loss: 0.7115 - val_acc: 0.8000\n",
      "Epoch 17/50\n",
      " - 5s - loss: 0.5465 - acc: 0.8395 - val_loss: 0.7081 - val_acc: 0.7990\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.5360 - acc: 0.8416 - val_loss: 0.7060 - val_acc: 0.8000\n",
      "Epoch 19/50\n",
      " - 5s - loss: 0.5265 - acc: 0.8446 - val_loss: 0.7075 - val_acc: 0.7980\n",
      "Epoch 20/50\n",
      " - 5s - loss: 0.5163 - acc: 0.8465 - val_loss: 0.6987 - val_acc: 0.8010\n",
      "Epoch 21/50\n",
      " - 5s - loss: 0.5080 - acc: 0.8516 - val_loss: 0.6985 - val_acc: 0.8020\n",
      "Epoch 22/50\n",
      " - 5s - loss: 0.4950 - acc: 0.8528 - val_loss: 0.6997 - val_acc: 0.8020\n",
      "Epoch 23/50\n",
      " - 5s - loss: 0.4901 - acc: 0.8577 - val_loss: 0.6927 - val_acc: 0.8000\n",
      "Epoch 24/50\n",
      " - 5s - loss: 0.4770 - acc: 0.8562 - val_loss: 0.6910 - val_acc: 0.8030\n",
      "Epoch 25/50\n",
      " - 5s - loss: 0.4693 - acc: 0.8601 - val_loss: 0.6878 - val_acc: 0.8030\n",
      "Epoch 26/50\n",
      " - 5s - loss: 0.4584 - acc: 0.8644 - val_loss: 0.6963 - val_acc: 0.8041\n",
      "Epoch 27/50\n",
      " - 5s - loss: 0.4507 - acc: 0.8647 - val_loss: 0.6802 - val_acc: 0.8010\n",
      "Epoch 28/50\n",
      " - 5s - loss: 0.4428 - acc: 0.8662 - val_loss: 0.6843 - val_acc: 0.8020\n",
      "Epoch 29/50\n",
      " - 5s - loss: 0.4353 - acc: 0.8714 - val_loss: 0.6867 - val_acc: 0.8061\n",
      "Epoch 30/50\n",
      " - 5s - loss: 0.4252 - acc: 0.8732 - val_loss: 0.6919 - val_acc: 0.8010\n",
      "Epoch 31/50\n",
      " - 5s - loss: 0.4281 - acc: 0.8726 - val_loss: 0.6991 - val_acc: 0.8010\n",
      "Epoch 32/50\n",
      " - 5s - loss: 0.4100 - acc: 0.8778 - val_loss: 0.6818 - val_acc: 0.8030\n",
      "Epoch 33/50\n",
      " - 5s - loss: 0.4036 - acc: 0.8796 - val_loss: 0.6803 - val_acc: 0.8010\n",
      "Epoch 34/50\n",
      " - 5s - loss: 0.3946 - acc: 0.8814 - val_loss: 0.6907 - val_acc: 0.7990\n",
      "Epoch 35/50\n",
      " - 5s - loss: 0.3888 - acc: 0.8875 - val_loss: 0.6948 - val_acc: 0.8000\n",
      "Epoch 36/50\n",
      " - 5s - loss: 0.3825 - acc: 0.8857 - val_loss: 0.6896 - val_acc: 0.8000\n",
      "Epoch 37/50\n",
      " - 5s - loss: 0.3735 - acc: 0.8896 - val_loss: 0.7085 - val_acc: 0.7980\n",
      "Epoch 38/50\n",
      " - 5s - loss: 0.3725 - acc: 0.8890 - val_loss: 0.7041 - val_acc: 0.7990\n",
      "Epoch 39/50\n",
      " - 5s - loss: 0.3619 - acc: 0.8945 - val_loss: 0.7004 - val_acc: 0.8010\n",
      "Epoch 40/50\n",
      " - 5s - loss: 0.3557 - acc: 0.8954 - val_loss: 0.7033 - val_acc: 0.8030\n",
      "Epoch 41/50\n",
      " - 5s - loss: 0.3488 - acc: 0.8951 - val_loss: 0.7219 - val_acc: 0.8010\n",
      "Epoch 42/50\n",
      " - 5s - loss: 0.3437 - acc: 0.8984 - val_loss: 0.7181 - val_acc: 0.8000\n",
      "Epoch 43/50\n",
      " - 6s - loss: 0.3377 - acc: 0.8972 - val_loss: 0.6927 - val_acc: 0.8000\n",
      "Epoch 44/50\n",
      " - 5s - loss: 0.3324 - acc: 0.9009 - val_loss: 0.7114 - val_acc: 0.8000\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.3260 - acc: 0.9042 - val_loss: 0.7159 - val_acc: 0.7970\n",
      "Epoch 46/50\n",
      " - 5s - loss: 0.3199 - acc: 0.9064 - val_loss: 0.6985 - val_acc: 0.7949\n",
      "Epoch 47/50\n",
      " - 5s - loss: 0.3144 - acc: 0.9073 - val_loss: 0.7086 - val_acc: 0.7980\n",
      "Epoch 48/50\n",
      " - 5s - loss: 0.3114 - acc: 0.9088 - val_loss: 0.7046 - val_acc: 0.8041\n",
      "Epoch 49/50\n",
      " - 5s - loss: 0.3050 - acc: 0.9127 - val_loss: 0.7094 - val_acc: 0.7949\n",
      "Epoch 50/50\n",
      " - 5s - loss: 0.2998 - acc: 0.9161 - val_loss: 0.7182 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[tweet_sentence_input, pattern_input], outputs=x)\n",
    "optimizer = optimizers.Adam(lr=0.0001)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])  \n",
    "history = model.fit(X_train, dummy_y_train, batch_size=32, #class_weight=class_weights,\n",
    "          epochs=50, verbose=2, validation_data=(X_dev, dummy_y_dev))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834/834 [==============================] - 0s 394us/step\n",
      "[0.7475488675441102, 0.7889688014984131]\n",
      "834/834 [==============================] - 0s 403us/step\n",
      "Test Set Accuracy: 78.90%\n",
      " Gold Standard Test:  [7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 3. 7. 7.\n",
      " 7. 4. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 3. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 4. 7. 7. 7. 1. 7. 1. 1. 7. 1. 4.\n",
      " 7. 7. 7. 7. 1. 1. 3. 7. 1. 7. 7. 7. 7. 7. 3. 7. 7. 7. 7. 6. 7. 7. 7. 1.\n",
      " 7. 7. 1. 7. 1. 1. 1. 1. 7. 0. 3. 7. 7. 6. 6. 7. 0. 6. 0. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 1. 7. 7. 7. 7. 7. 7. 1. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 6. 7. 6. 7. 6. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 5. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 6. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 4. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 2. 7. 7. 6. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 6. 7. 7. 7. 7.\n",
      " 7. 7. 7. 3. 3. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 3. 7. 7. 7. 7. 7. 1.\n",
      " 7. 7. 7. 7. 7. 7. 4. 4. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 5. 7.\n",
      " 7. 7. 7. 7. 7. 7. 1. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 0. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 4. 5. 4. 4. 4. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 3. 7. 4. 7. 5. 0. 7. 7. 7. 7. 7. 0. 7. 7. 7. 7. 7. 7. 7. 7. 1. 7.\n",
      " 7. 7. 7. 5. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 1. 7.\n",
      " 7. 7. 7. 7. 7. 3. 1. 7. 7. 7. 7. 7. 7. 3. 7. 7. 7. 7. 1. 4. 1. 0. 7. 7.\n",
      " 7. 7. 7. 7. 7. 0. 7. 3. 0. 0. 6. 0. 4. 5. 5. 7. 7. 0. 4. 5. 7. 4. 7. 7.\n",
      " 7. 7. 3. 3. 4. 4. 7. 7. 7. 7. 1. 7. 7. 7. 4. 2. 5. 4. 4. 7. 7. 7. 4. 7.\n",
      " 7. 0. 7. 4. 7. 7. 4. 2. 7. 7. 7. 7. 7. 3. 3. 7. 7. 7. 7. 7. 1. 7. 1. 7.\n",
      " 1. 3. 3. 7. 7. 7. 1. 7. 7. 1. 1. 2. 1. 7. 7. 1. 7. 7. 7. 0. 1. 1. 7. 3.\n",
      " 7. 1. 7. 7. 7. 7. 7. 1. 7. 0. 7. 3. 1. 3. 7. 1. 7. 7. 7. 7. 7. 1. 1. 0.\n",
      " 1. 7. 1. 1. 2. 1. 1. 3. 1. 7. 1. 7. 7. 7. 7. 7. 7. 7. 1. 1. 1. 1. 7. 1.\n",
      " 6. 7. 7. 7. 7. 1. 7. 7. 1. 7. 7. 1. 1. 1. 7. 7. 3. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 5. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 5. 5. 5. 5. 5. 5. 5. 7. 5. 5. 7. 5.\n",
      " 7. 7. 5. 5. 5. 5. 5. 7. 5. 7. 5. 5. 5. 5. 7. 5. 5. 7. 5. 7. 7. 6. 7. 5.\n",
      " 5. 7. 5. 5. 7. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 7. 7. 7. 3. 7. 3. 3. 7. 7.\n",
      " 0. 7. 7. 4. 7. 7. 7. 6. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.]\n",
      "Test Predictions:  [7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 4 7 7 7 5 7 4 7 7 1 4 7 7\n",
      " 7 7 3 6 7 7 3 4 6 1 7 7 7 7 7 7 7 6 7 7 7 4 3 7 3 7 7 7 7 7 7 3 7 7 7 7 7\n",
      " 7 6 7 7 7 7 7 7 7 7 7 7 7 0 7 7 7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 6 7 6 3 7 7 7 7 7 7 7 7 7 7\n",
      " 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 6 0 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 4 7 4 7 7 7 7 4 7 7 7 7 7 7 6 7 7 7 7 7 7 7 3 7 7 7 7 4 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 5 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 3 6 5 3 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 6 7\n",
      " 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 6 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 6 7 6 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 3 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 5\n",
      " 5 4 4 4 7 7 7 7 7 7 7 7 7 7 7 7 4 7 5 5 7 7 7 7 7 6 7 7 7 7 7 7 7 7 4 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 7 7 7 7 7 3 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 4 0 0 7 7 7 7 7 7 7 5 7 7 0 0 7 1 7 7 5 7 7 7 7 5 7 4 7 7 7 7 3\n",
      " 7 7 7 7 7 7 7 2 7 7 7 4 7 5 4 7 7 7 7 7 7 7 3 7 4 7 7 7 6 7 7 7 7 7 1 7 7\n",
      " 7 7 7 7 7 7 7 7 0 3 0 7 3 7 5 7 7 6 3 6 0 6 3 6 7 7 7 0 5 5 3 5 7 0 7 7 7\n",
      " 7 7 0 7 0 7 3 6 7 7 3 7 7 7 7 6 5 6 6 0 7 6 4 6 7 7 5 3 7 7 7 7 7 7 7 7 7\n",
      " 6 1 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 3 7 7 7 7 3 7 7 7 7 7 7 7 7 5 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 7 5 7 7 7 7 7 7\n",
      " 7 5 7 4 7 7 5 7 7 5 7 7 5 7 7 5 7 5 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 5 7 7 7\n",
      " 7 7 7 7 5 7 7 7 7 7 7 7 7 7 7 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "Train Predictions:  [7 7 7 ... 7 7 7]\n",
      "Dev Predictions:  [7 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 4 7 4 0 6 7 5 7 7 6 7 7 7 7 7 6 7 7 1\n",
      " 7 6 7 7 7 7 7 0 7 7 7 7 6 7 7 7 7 4 7 7 7 7 7 7 6 7 7 7 7 7 7 7 3 6 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 7\n",
      " 6 4 7 7 7 0 0 7 7 7 4 7 6 7 7 7 7 4 7 7 7 3 1 3 7 3 7 7 7 7 7 7 3 7 5 7 3\n",
      " 7 7 7 7 5 3 7 7 7 7 3 7 7 7 7 7 7 7 7 7 1 7 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 6 7 0 7 7 7 7 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 4 7 7 7 4 7 7 7 7 7 7 7 7 4 7 7 7 0 4 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 4 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 5 7 1 4 7\n",
      " 7 7 7 4 7 7 7 4 7 4 7 7 7 7 4 7 6 4 7 7 4 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 5\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 6 7 7 7 7 0 7 7 7 1 7 7 7 6 7 7 6\n",
      " 7 7 7 7 7 7 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 7 7 7 7 7 7 7 3 7 7\n",
      " 7 7 7 3 6 7 7 7 7 7 7 7 7 1 7 3 7 7 7 7 7 7 7 5 3 7 7 7 7 7 7 5 7 7 7 7 7\n",
      " 7 1 7 7 0 7 7 7 7 7 1 7 7 7 7 6 7 7 7 7 7 7 1 7 7 7 3 4 7 7 4 5 7 2 7 4 7\n",
      " 5 4 7 5 7 7 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 5 7 7 7 7 7\n",
      " 7 7 7 3 7 7 7 7 7 7 5 7 3 7 7 6 7 3 2 7 7 7 7 7 7 5 7 7 7 5 7 7 7 7 7 7 7\n",
      " 7 7 7 5 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 4 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 6 7 7 7 7 7 7 7 7 6 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 4 7 7 7\n",
      " 7 7 7 7 7 5 0 7 0 7 7 5 5 7 5 7 7 7 7 7 5 5 5 5 4 0 7 7 0 0 7 7 0 7 7 4 7\n",
      " 3 5 7 4 4 0 5 5 5 5 4 5 7 7 7 7 5 7 5 7 7 7 7]\n",
      " Macro F1-Score:  0.34055308595870426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.evaluate(X_test, dummy_y_test))\n",
    "test_loss_content, test_acc_content = model.evaluate(X_test, dummy_y_test)\n",
    "print(\"Test Set Accuracy: %0.2f%%\"%(test_acc_content*100))\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_test_classes_pred = y_pred_test.argmax(axis=-1)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_train_classes_pred = y_pred_train.argmax(axis=-1)\n",
    "y_pred_dev = model.predict(X_dev)\n",
    "y_dev_classes_pred = y_pred_dev.argmax(axis=-1)\n",
    "print(\" Gold Standard Test: \",y_test)\n",
    "print(\"Test Predictions: \",y_test_classes_pred)\n",
    "print(\"Train Predictions: \",y_train_classes_pred)\n",
    "print(\"Dev Predictions: \",y_dev_classes_pred)\n",
    "print(\" Macro F1-Score: \",f1_score(y_test, y_test_classes_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.29      0.32        17\n",
      "         1.0       0.40      0.04      0.07        56\n",
      "         2.0       0.00      0.00      0.00         5\n",
      "         3.0       0.24      0.27      0.25        26\n",
      "         4.0       0.54      0.54      0.54        24\n",
      "         5.0       0.50      0.32      0.39        47\n",
      "         6.0       0.17      0.43      0.24        14\n",
      "         7.0       0.88      0.95      0.91       645\n",
      "\n",
      "    accuracy                           0.79       834\n",
      "   macro avg       0.39      0.35      0.34       834\n",
      "weighted avg       0.77      0.79      0.76       834\n",
      "\n",
      "[[  5   1   0   2   0   3   4   2]\n",
      " [  7   2   1   7   4   6  10  19]\n",
      " [  0   0   0   0   1   0   3   1]\n",
      " [  1   1   0   7   0   2   0  15]\n",
      " [  0   0   0   0  13   1   0  10]\n",
      " [  0   0   0   0   1  15   0  31]\n",
      " [  0   0   0   0   1   0   6   7]\n",
      " [  1   1   0  13   4   3  13 610]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.13      0.08      0.10        24\n",
      "         1.0       0.20      0.05      0.08        43\n",
      "         2.0       0.33      0.05      0.08        22\n",
      "         3.0       0.36      0.22      0.28        36\n",
      "         4.0       0.46      0.44      0.45        39\n",
      "         5.0       0.64      0.48      0.55        48\n",
      "         6.0       0.12      0.38      0.18         8\n",
      "         7.0       0.87      0.96      0.91       765\n",
      "\n",
      "    accuracy                           0.80       985\n",
      "   macro avg       0.39      0.33      0.33       985\n",
      "weighted avg       0.76      0.80      0.77       985\n",
      "\n",
      "[[  2   2   0   2   1   2   5  10]\n",
      " [  3   2   1   5   1   0   5  26]\n",
      " [  1   3   1   0   1   0   4  12]\n",
      " [  1   1   0   8   0   5   0  21]\n",
      " [  3   1   0   0  17   3   1  14]\n",
      " [  3   0   0   0   2  23   1  19]\n",
      " [  0   0   0   0   2   0   3   3]\n",
      " [  2   1   1   7  13   3   6 732]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, y_test_classes_pred))\n",
    "print(confusion_matrix(y_test, y_test_classes_pred))\n",
    "\n",
    "#type(y_pred_dev)\n",
    "print(classification_report(y_dev, y_dev_classes_pred))\n",
    "print(confusion_matrix(y_dev, y_dev_classes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
